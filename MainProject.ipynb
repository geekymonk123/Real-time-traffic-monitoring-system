{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geekymonk123/Real-time-traffic-monitoring-system/blob/main/MainProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U kaleido"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tunZTNkXHcH8",
        "outputId": "ad80bd3d-822b-4713-c534-8247a2fbf822"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.10/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "KUg-SEBKL5YH",
        "outputId": "43297fba-d52b-4e86-f9e5-769d8e4d6c95"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'easyocr'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d14271282ebd>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfasterrcnn_resnet50_fpn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0measyocr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'easyocr'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "  #!pip install easyocr\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import csv\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "import torchvision.transforms as T\n",
        "import easyocr\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load the pre-trained Faster R-CNN model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define a transformation for the input image\n",
        "transform = T.Compose([T.ToTensor()])\n",
        "\n",
        "# Initialize EasyOCR for license plate OCR\n",
        "reader = easyocr.Reader(['en'], gpu=True)\n",
        "\n",
        "# Function to perform object detection and classification\n",
        "def detect_objects(image):\n",
        "    input_tensor = transform(image).to(device)\n",
        "    input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input_batch)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Function to calculate the absolute speed of a vehicle\n",
        "def calculate_absolute_speed(prev_position, current_position, time_diff, focal_length):\n",
        "    distance_pixels = np.linalg.norm(current_position - prev_position)\n",
        "    distance_meters = (focal_length * 2) / distance_pixels\n",
        "    speed_mps = distance_meters / time_diff\n",
        "    speed_kph = speed_mps * 3.6\n",
        "    return speed_kph\n",
        "\n",
        "# Function to perform OCR on license plates\n",
        "def ocr_license_plate(image, coordinates):\n",
        "    x, y, w, h = int(coordinates[0]), int(coordinates[1]), int(coordinates[2]), int(coordinates[3])\n",
        "    plate_image = image[y:h, x:w]\n",
        "\n",
        "    gray_plate = cv2.cvtColor(plate_image, cv2.COLOR_RGB2GRAY)\n",
        "    result = reader.readtext(gray_plate, detail=0)\n",
        "    license_plate_number = \"\"\n",
        "    confidence_score = 0.0\n",
        "\n",
        "    for res in result:\n",
        "\n",
        "        if len(res) > 6:\n",
        "            license_plate_number = res\n",
        "\n",
        "\n",
        "    return license_plate_number, confidence_score\n",
        "\n",
        "# Function to process video frames and write results to CSV\n",
        "def process_frame(frame, prev_positions, prev_time, writer, video_capture, focal_length):\n",
        "    predictions = detect_objects(frame)\n",
        "\n",
        "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "    labels = predictions[0]['labels'].cpu().numpy()\n",
        "\n",
        "    current_time = time.time()\n",
        "    time_diff = current_time - prev_time\n",
        "\n",
        "    for box, label in zip(boxes, labels):\n",
        "        box_center = np.array([(box[0] + box[2]) / 2, (box[1] + box[3]) / 2])\n",
        "        if label in prev_positions:\n",
        "            prev_position = prev_positions[label]\n",
        "            speed = calculate_absolute_speed(prev_position, box_center, time_diff, focal_length)\n",
        "\n",
        "            license_plate_number, confidence_score = ocr_license_plate(frame, box)\n",
        "\n",
        "            # Write license plate number and confidence level to CSV\n",
        "            if license_plate_number:\n",
        "                writer.writerow({'Frame': int(video_capture.get(cv2.CAP_PROP_POS_FRAMES)),\n",
        "                                 'Label': label,\n",
        "                                 'License Plate': license_plate_number,\n",
        "                                 'Confidence Score': confidence_score,\n",
        "                                 'X': box_center[0],\n",
        "                                 'Y': box_center[1],\n",
        "                                 'Speed (km/h)': speed})\n",
        "\n",
        "                # Draw bounding box around the detected object\n",
        "                cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
        "\n",
        "                # Display the license plate number and speed on the frame\n",
        "                text = f'{license_plate_number} {speed:.2f} km/h'\n",
        "                cv2.putText(frame, text, (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "        prev_positions[label] = box_center\n",
        "\n",
        "    return frame\n",
        "\n",
        "# Function to process real-time video stream\n",
        "def process_realtime_video(video_path, focal_length, output_csv):\n",
        "    video_capture = cv2.VideoCapture(video_path)\n",
        "    prev_positions = {}\n",
        "    prev_time = time.time()\n",
        "\n",
        "    # Get video frame properties\n",
        "    frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Open CSV file for writing license plate predictions\n",
        "    with open(output_csv, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['Frame', 'Label', 'License Plate', 'Confidence Score', 'X', 'Y', 'Speed (km/h)']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        while True:\n",
        "            ret, frame = video_capture.read()\n",
        "\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            processed_frame = process_frame(frame, prev_positions, prev_time, writer, video_capture, focal_length)\n",
        "\n",
        "            cv2_imshow(processed_frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "            prev_time = time.time()\n",
        "\n",
        "    video_capture.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Run the real-time video processing function\n",
        "video_path = '/content/drive/MyDrive/Colab Notebooks/Dataset/pexels_videos_2103099 (2160p) (1) (online-video-cutter.com).mp4'\n",
        "focal_length = 1000.0  # Adjust as per your camera setup\n",
        "output_csv = '/content/drive/MyDrive/Colab Notebooks/Dataset/Untitled spreadsheet - Sheet1.csv'\n",
        "process_realtime_video(video_path, focal_length, output_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYqBouxWrNMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6iwbf5_GpPi"
      },
      "source": [
        "Object Motion detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbx9w8xZ_XMS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision.transforms import functional as F\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "# Define the class labels for Faster R-CNN\n",
        "faster_rcnn_classes = [\n",
        "    'background', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n",
        "    'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
        "    'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "    'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',\n",
        "    'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\n",
        "    'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "# Load the pre-trained Faster R-CNN model\n",
        "fasterrcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "fasterrcnn_model.eval()\n",
        "\n",
        "# Function to perform inference using Faster R-CNN\n",
        "def detect_objects_fasterrcnn(frame, model):\n",
        "    image = Image.fromarray(frame)\n",
        "    image_tensor = F.to_tensor(image)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model([image_tensor])\n",
        "\n",
        "    return outputs[0]\n",
        "\n",
        "# Function to draw bounding boxes and labels\n",
        "def draw_boxes(frame, detections, classes):\n",
        "    for box, label, score in zip(detections['boxes'], detections['labels'], detections['scores']):\n",
        "        if score > 0.5:\n",
        "            x1, y1, x2, y2 = map(int, box.tolist())\n",
        "            label = f'{classes[label.item()]}: {score:.2f}'\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "    return frame\n",
        "\n",
        "# Open the video file\n",
        "video_path = '/content/drive/MyDrive/Colab Notebooks/Dataset/Video.mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Prepare for saving output video\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter('output.avi', fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Perform inference using Faster R-CNN\n",
        "    detections_fasterrcnn = detect_objects_fasterrcnn(frame, fasterrcnn_model)\n",
        "\n",
        "    # Draw bounding boxes and labels\n",
        "    frame_fasterrcnn = draw_boxes(frame.copy(), detections_fasterrcnn, faster_rcnn_classes)\n",
        "\n",
        "    # Write the frame with detections to the output video\n",
        "    out.write(frame_fasterrcnn)\n",
        "\n",
        "    # Display the frame\n",
        "    cv2_imshow(frame_fasterrcnn)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release video capture and writer objects\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ugWdV_vG4zQ"
      },
      "outputs": [],
      "source": [
        "!pip install pytesseract\n",
        "!apt-get install tesseract-ocr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06J7Mf5AHaFN"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "!pip install easyocr\n",
        "import easyocr\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Initialize EasyOCR reader\n",
        "reader = easyocr.Reader(['en'])\n",
        "\n",
        "# Function to perform OCR using Tesseract\n",
        "def ocr_tesseract(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    text = pytesseract.image_to_string(gray).strip()\n",
        "    return text\n",
        "\n",
        "# Function to perform OCR using EasyOCR\n",
        "def ocr_easyocr(image):\n",
        "    result = reader.readtext(image)\n",
        "    text = ' '.join([res[1] for res in result])\n",
        "    return text\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Remove extra whitespace, newlines, and non-alphanumeric characters\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text.lower()\n",
        "\n",
        "# Function to calculate similarity ratio\n",
        "def calculate_similarity(gt_text, pred_text):\n",
        "    return SequenceMatcher(None, gt_text, pred_text).ratio()\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(gt_texts, pred_texts):\n",
        "    correct = sum([1 for gt, pred in zip(gt_texts, pred_texts) if gt == pred])\n",
        "    return correct / len(gt_texts)\n",
        "\n",
        "# Sample images and ground truth texts\n",
        "image_paths = [\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB1.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB11.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB12.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB13.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB14.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB15.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB16.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB17.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB18.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB19.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB20.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB21.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB22.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB23.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB25.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB26.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB27.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB28.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB2.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB3.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB4.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB5.jpg',\n",
        "      '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB6.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB8.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/WB9.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/DN1.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/DN2.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/DN3.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/DN4.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/DN5.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/DN6.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/DN7.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/DN8.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/RJ2.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/RJ4.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/RJ5.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/RJ6.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/RJ7.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/RJ9.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/RJ10.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/TN1.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/TN2.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/TN3.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/TN4.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/TN5.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/TN6.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/TN7.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/TN8.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/TN9.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/TN10.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP1.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP2.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP3.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP4.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP5.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP8.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP10.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP11.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP12.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP14.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP15.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP16.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP17.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP19.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP20.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP21.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP22.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP23.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP24.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP25.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP26.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP27.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP28.jpg',\n",
        "     '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/UP29.jpg'\n",
        "\n",
        "]\n",
        "gt_texts = [\n",
        "    'WB26T6688','WB08A4101','WB26S5093',\n",
        "    'WB26K2760','WB36G6682',\n",
        "    'WB12AX2987','WB06J9177','WB02AJ4534',\n",
        "    'WB06J2192','WB061061','WB171717',\n",
        "    'WB02W9S3S','WB20H0271','WB20AG9041',\n",
        "    'WB06H2237','WB02A03017','WB02AD4162',\n",
        "    'WB02AM8548','WB02M8447','WB70G3763','WB02X8795',\n",
        "    'WB06G4120','WB02AH4655','WB06J4432','WB02AA5580',\n",
        "    'DN09H2191','DN09J0548','DN09H0968','DN09J0032','DN09D2724',\n",
        "    'DN0901438','DN0909704','DN09C0868',\n",
        "    'RJ14CW4115','RJ20CD5118','RJ14CF5807','RJ23CB2272',\n",
        "    'RJ23CB4320','RJ20UB0137','RJ14CM3730',\n",
        "    'TN63BV7954','TN76V1978','TN02AU9295','TN10AR0131',\n",
        "    'TN47T4464','TN19F0816','TN07BT5778','TN07AP0659','TN19Q0835','TN69AJ4455',\n",
        "    'UP57H8173','UP85BN4203','UP80FA1666','UP16U3849','UP32EC5577','UP16AJ0704','UP78DB3730',\n",
        "    'UP53AK1161','UP14CV7920','UP16BD4866','UP79L0567','UP32KW7325','UP32CX3400','UP45Q3028',\n",
        "    'UP14AK8604','UP52AH9705','UP11AY0646','UP78ED0449','UP27Z7322','UP32BF3577','UP14AX9754',\n",
        "    'UP70ET7816','UP32HY6077','UP14AF3804'\n",
        "    # Add corresponding ground truth texts\n",
        "]\n",
        "\n",
        "tesseract_texts = []\n",
        "easyocr_texts = []\n",
        "tesseract_accuracies = []\n",
        "easyocr_accuracies = []\n",
        "for image_path,gt_text in zip(image_paths,gt_texts):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Perform OCR using Tesseract\n",
        "    tesseract_text = ocr_tesseract(image)\n",
        "    tesseract_texts.append(tesseract_text)\n",
        "\n",
        "    # Perform OCR using EasyOCR\n",
        "    easyocr_text = ocr_easyocr(image)\n",
        "    easyocr_texts.append(easyocr_text)\n",
        "     # Preprocess texts\n",
        "    gt_text_clean = preprocess_text(gt_text)\n",
        "    tesseract_text_clean = preprocess_text(tesseract_text)\n",
        "    easyocr_text_clean = preprocess_text(easyocr_text)\n",
        "\n",
        "    # Calculate similarity ratios\n",
        "    tesseract_similarity = calculate_similarity(gt_text_clean, tesseract_text_clean)\n",
        "    easyocr_similarity = calculate_similarity(gt_text_clean, easyocr_text_clean)\n",
        "\n",
        "    # Append accuracies (similarity ratios)\n",
        "    tesseract_accuracies.append(tesseract_similarity)\n",
        "    easyocr_accuracies.append(easyocr_similarity)\n",
        "\n",
        "# Calculate average accuracies\n",
        "average_tesseract_accuracy = np.mean(tesseract_accuracies)\n",
        "average_easyocr_accuracy = np.mean(easyocr_accuracies)\n",
        "\n",
        "print(\"Tesseract OCR Accuracy:\", average_tesseract_accuracy * 100)\n",
        "print(\"EasyOCR Accuracy:\", average_easyocr_accuracy * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAJqxJZ-bFTf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load a pre-trained Faster R-CNN model\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Define COCO class names (first class is the background)\n",
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter',\n",
        "    'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n",
        "    'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "    'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
        "    'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n",
        "    'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
        "    'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]\n",
        "\n",
        "# Define the image transformation\n",
        "transform = T.Compose([\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "# Load the image\n",
        "image_path = '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/Classification.jpg'\n",
        "image = Image.open(image_path)\n",
        "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    predictions = model(image_tensor)\n",
        "\n",
        "# Get the predictions\n",
        "pred_boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "pred_scores = predictions[0]['scores'].cpu().numpy()\n",
        "pred_classes = predictions[0]['labels'].cpu().numpy()\n",
        "\n",
        "# Filter out low confidence scores\n",
        "threshold = 0.5\n",
        "filtered_boxes = pred_boxes[pred_scores >= threshold]\n",
        "filtered_classes = pred_classes[pred_scores >= threshold]\n",
        "\n",
        "# Draw bounding boxes on the original image\n",
        "output_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "for i, box in enumerate(filtered_boxes):\n",
        "    if COCO_INSTANCE_CATEGORY_NAMES[filtered_classes[i]] == 'car':\n",
        "        x1, y1, x2, y2 = box\n",
        "        cv2.rectangle(output_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "        label = f'{COCO_INSTANCE_CATEGORY_NAMES[filtered_classes[i]]}: {pred_scores[i]:.2f}'\n",
        "        cv2.putText(output_image, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(output_image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Optionally save the output image\n",
        "output_image_path = '/content/drive/MyDrive/Colab Notebooks/Dataset/output_car_image.jpg'\n",
        "cv2.imwrite(output_image_path, cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load a pre-trained Faster R-CNN model\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Define COCO class names (first class is the background)\n",
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter',\n",
        "    'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n",
        "    'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "    'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
        "    'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n",
        "    'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
        "    'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]\n",
        "\n",
        "# Define the image transformation\n",
        "transform = T.Compose([\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "# Load the image\n",
        "image_path = '/content/drive/MyDrive/Colab Notebooks/Dataset/Images/Classification.jpg'\n",
        "image = Image.open(image_path)\n",
        "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    predictions = model(image_tensor)\n",
        "\n",
        "# Get the predictions\n",
        "pred_boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "pred_scores = predictions[0]['scores'].cpu().numpy()\n",
        "pred_classes = predictions[0]['labels'].cpu().numpy()\n",
        "\n",
        "# Filter out low confidence scores\n",
        "threshold = 0.94\n",
        "filtered_boxes = pred_boxes[(pred_scores >= threshold) & (pred_classes == COCO_INSTANCE_CATEGORY_NAMES.index('car'))]\n",
        "filtered_scores = pred_scores[(pred_scores >= threshold) & (pred_classes == COCO_INSTANCE_CATEGORY_NAMES.index('car'))]\n",
        "filtered_classes = pred_classes[(pred_scores >= threshold) & (pred_classes == COCO_INSTANCE_CATEGORY_NAMES.index('car'))]\n",
        "\n",
        "# Draw bounding boxes on the original image\n",
        "output_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "for i, box in enumerate(filtered_boxes):\n",
        "    x1, y1, x2, y2 = box\n",
        "    cv2.rectangle(output_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "    label = f'{COCO_INSTANCE_CATEGORY_NAMES[filtered_classes[i]]}: {filtered_scores[i]:.2f}'\n",
        "    cv2.putText(output_image, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(output_image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Optionally save the output image\n",
        "output_image_path = '/content/drive/MyDrive/Colab Notebooks/Dataset/output_car_image.jpg'\n",
        "cv2.imwrite(output_image_path, cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR))\n"
      ],
      "metadata": {
        "id": "kD7kftYYsFCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the image with bounding boxes\n",
        "image_path = '/content/drive/MyDrive/Colab Notebooks/Dataset/download (37).png'\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Create a mask where the green lines are located\n",
        "lower_green = np.array([0, 255, 0], dtype=np.uint8)\n",
        "upper_green = np.array([0, 255, 0], dtype=np.uint8)\n",
        "mask = cv2.inRange(image, lower_green, upper_green)\n",
        "\n",
        "# Dilate the mask to cover the text and bounding box\n",
        "kernel = np.ones((3, 3), np.uint8)\n",
        "mask = cv2.dilate(mask, kernel, iterations=1)\n",
        "\n",
        "# Inpaint the image using the mask\n",
        "inpainted_image = cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)\n",
        "\n",
        "# Convert the image from BGR to RGB\n",
        "output_image = cv2.cvtColor(inpainted_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(output_image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Optionally save the output image\n",
        "output_image_path = '/mnt/data/output_image_without_boxes.png'\n",
        "cv2.imwrite(output_image_path, cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR))\n"
      ],
      "metadata": {
        "id": "QstOB6R72ivy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iccnebtd3e6x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}